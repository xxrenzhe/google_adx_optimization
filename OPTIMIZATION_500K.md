# 50万行数据处理优化方案

## 实现概述

本方案针对1C2G容器配置，优化了Google ADX系统以支持50万行数据的上传、存储、读取和分析功能。

## 核心优化

### 1. 架构简化
- **放弃数据库存储**：使用文件系统存储原始文件和分析结果
- **独立分析**：每次上传独立分析，不读取历史数据
- **流式处理**：避免一次性加载所有数据到内存

### 2. 内存优化
- **Node.js内存限制**：增加到1.5GB
- **分批处理**：每10万行强制GC一次
- **及时清理**：处理完成后立即清空Map数据结构
- **样本限制**：只保存3000行样本数据

### 3. 性能优化
- **异步处理**：文件上传后立即返回，后台异步分析
- **进度跟踪**：实时更新处理进度
- **自动清理**：24小时后自动清理过期文件

## 文件结构

```
uploads/          # 原始CSV文件
├── {fileId}.csv
results/          # 分析结果和状态
├── {fileId}.json  # 分析结果
└── {fileId}.status # 处理状态

app/api/
├── upload-optimized/route.ts    # 优化的上传API
├── result/[fileId]/route.ts     # 结果查询API
└── cleanup/route.ts             # 文件清理API

components/
└── upload-optimized.tsx         # 优化的上传组件

app/page-optimized.tsx           # 优化的首页
```

## API端点

### 1. 文件上传
```
POST /api/upload-optimized
- 支持最大200MB文件
- 异步处理，立即返回fileId
- 创建状态文件跟踪进度
```

### 2. 结果查询
```
GET /api/result/{fileId}
- 返回处理状态或结果
- 支持进度轮询
- 自动过期处理
```

### 3. 文件清理
```
GET /api/cleanup
- 自动清理24小时前的文件
- 返回清理统计

DELETE /api/cleanup?fileId={id}
- 手动删除特定文件
```

## 数据处理流程

1. **文件上传** → 保存到`uploads/`目录
2. **创建状态** → 创建`.status`文件
3. **流式读取** → 逐行解析CSV
4. **实时聚合** → 使用Map结构聚合数据
5. **保存结果** → 生成`.json`结果文件
6. **更新状态** → 标记为完成
7. **清理内存** → 释放所有数据结构

## 性能预期

- **文件上传**：5-10秒（取决于网络）
- **数据处理**：30-40秒（50万行）
- **内存使用**：峰值700-800MB
- **响应时间**：< 1秒（预聚合后）

## 监控指标

- 处理进度百分比
- 已处理行数
- 内存使用情况
- 文件清理统计

## 使用方法

1. 启动优化版本：
```bash
npm run start:optimized
# 或使用
./start-optimized.sh
```

2. 访问优化页面：
```
http://localhost:3000/page-optimized
```

3. 上传CSV文件（最大200MB）

4. 查看实时分析结果

## 测试方法

运行测试脚本：
```bash
./test-500k.sh
```

这将：
1. 生成50万行测试数据
2. 启动服务器
3. 上传文件
4. 监控处理进度
5. 报告性能指标

## 注意事项

1. **文件大小限制**：单个文件最大200MB
2. **并发限制**：建议同时处理不超过3个文件
3. **内存监控**：定期监控内存使用情况
4. **磁盘空间**：定期清理上传目录
5. **网络超时**：大文件上传可能需要调整超时设置

## 扩展性

如需支持更大文件：
1. 增加容器内存配置
2. 实现文件分片上传
3. 使用更高效的数据结构
4. 考虑专门的OLAP数据库（如ClickHouse）